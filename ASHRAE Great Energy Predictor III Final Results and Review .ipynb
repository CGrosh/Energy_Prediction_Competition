{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below are my best models and Preprocessing steps for the ASHRAE Great Energy predictor Kaggle Competition \n",
    "## The goal of the competition was to predict the Energy consumption of buildings for the given test set of buildings \n",
    "## The provided data consisted of 3 data files for training that were joined based on building_id, site_id, and timestamp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from util import *\n",
    "import time \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.svm import LinearSVR\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA \n",
    "import pickle \n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb \n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runner function, when called will complete the whole preprocessing of the data \n",
    "# and return the preprocessed and split up data \n",
    "def run_it(file, rows):\n",
    "    data = get_data(file, rows)\n",
    "    x_train, x_test, y_train, y_test = process_data(data, 0.95)\n",
    "    return x_train, x_test, y_train, y_test \n",
    "\n",
    "# This function creates new features with the existing data, One-Hot Encodes, \n",
    "# and then splits up the Features and Target variable\n",
    "def feature_engine(df):\n",
    "    df['Year'] = df['timestamp'].str[:4]\n",
    "    df['Year'] = df['Year'].astype('int64')\n",
    "    df['building_age'] = df['Year'] - df['year_built']\n",
    "\n",
    "    df['Month'] = df['timestamp'].str[5:7]\n",
    "    df['Month'] = df['Month'].astype('int64')\n",
    "    df['Day'] = df['timestamp'].str[8:10]\n",
    "    df['Day'] = df['Day'].astype('int64')\n",
    "    \n",
    "    # Remove the unique identifiers and unneeded columns\n",
    "    cols = list(df.columns)\n",
    "    cols.remove('meter_reading')\n",
    "    cols.remove('Unnamed: 0')\n",
    "#     cols.remove('building_id')\n",
    "    cols.remove('timestamp')\n",
    "    cols.remove('year_built')\n",
    "#     cols.remove('site_id')\n",
    "    cols.remove('Year')\n",
    "#     cols.remove('Month')\n",
    "    \n",
    "    # One-hot encode based on the primary_use and meter type features \n",
    "    data_x = pd.get_dummies(df[cols], columns=['primary_use', 'meter', 'site_id'])\n",
    "    \n",
    "    # EDA showed a correlation between floor_count and square foot of the building, as you would expect \n",
    "    # This get the line gets the mean floor count based the binned square foot value and fills any null \n",
    "    # floor count spaces with that mean \n",
    "    data_x['binned_sqft'] = bin_sqft(data_x)\n",
    "    data_x['floor_count'] = data_x.groupby('binned_sqft')['floor_count'].transform(lambda x: x.fillna(x.mean()))\n",
    "    del data_x['binned_sqft']\n",
    "    \n",
    "    data_x['building_id_v2'] = data_x['building_id'].astype('category')\n",
    "    data_x['building_age'] = data_x.groupby('building_id_v2')['building_age'].transform(lambda x: x.fillna(x.mean()))\n",
    "    del data_x['building_id_v2']\n",
    "    del data_x['building_id']\n",
    "    # Create a column that bins the Square foot feature by interquartial range\n",
    "    # to be used for filling null values of the floor count feature \n",
    "\n",
    "    return data_x\n",
    "\n",
    "# Function to bin the Square foot feature based on interquartial range \n",
    "def bin_sqft(df):\n",
    "    vals = df['square_feet'].describe()\n",
    "    lst = []\n",
    "    for i in range(len(df)):\n",
    "        if df['square_feet'][i] >= vals['75%']:\n",
    "            lst.append('Top')\n",
    "        elif df['square_feet'][i] < vals['75%'] and df['square_feet'][i] >= vals['50%']:\n",
    "            lst.append(\"High\")\n",
    "        elif df['square_feet'][i] < vals['50%'] and df['square_feet'][i] >= vals['25%']:\n",
    "            lst.append('Middle')\n",
    "        elif df['square_feet'][i] < vals['25%']:\n",
    "            lst.append('Low')\n",
    "    return pd.Series(lst)\n",
    "\n",
    "# Function to read in a specified amount of the data \n",
    "def get_data(filename, row_num):\n",
    "    df = pd.read_csv(filename, nrows=row_num)\n",
    "    return df\n",
    "\n",
    "# Function to called during te initial run_it function \n",
    "# Calls the feature engine function to select the specific features and one-hot encode \n",
    "# Defines the preprocessing pipeline to fill nulls, standardize features, and selct important features \n",
    "def process_data(df, pca_level):\n",
    "    \n",
    "    # Call feature engine function to get features \n",
    "    data_x = feature_engine(df)\n",
    "\n",
    "    # data_x['building_id_v2'] = data_x['building_id'].astype('category')\n",
    "    # data_x['building_age'] = data_x.groupby('building_id_v2').transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    data_y = df['meter_reading']\n",
    "\n",
    "    # Preprocessing Pipeline \n",
    "    PP_Pipeline = Pipeline([\n",
    "        ('Imputer', SimpleImputer(missing_values=np.nan, strategy='mean')), \n",
    "        ('Scaler', preprocessing.MinMaxScaler()), \n",
    "        ('PCA', PCA(n_components=pca_level)),\n",
    "    ])\n",
    "    \n",
    "    # Train test split \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, \n",
    "                                        test_size=0.3, random_state=4)\n",
    "    \n",
    "    # Run the train and test features through the Pipeline \n",
    "    x_train_pp = PP_Pipeline.fit_transform(x_train)\n",
    "    x_test_pp = PP_Pipeline.transform(x_test)\n",
    "\n",
    "    # PipelineFile = open(\"PipelineFile\", \"wb\")\n",
    "    # pickle.dump(PP_Pipeline, PipelineFile)\n",
    "    # PipelineFile.close()\n",
    "\n",
    "    print('\\n')\n",
    "    print('Completed Preprocessing and Dimensionality Reduction')\n",
    "    print('\\n')\n",
    "\n",
    "    return x_train_pp, x_test_pp, y_train, y_test\n",
    "\n",
    "\n",
    "x_train_pp, x_test_pp, y_train, y_test = run_it('Final_Data.csv', 5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to Train XGBoost Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgrosh/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training took approx. 5.824481654167175 minutes\n",
      "RMSLE:  1.6101240105571846\n",
      "N_estimators : 25\n",
      "Max_Depth:  20\n",
      "Alpha Val:  11\n",
      "Learning Rate:  0.3\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "n_est = 25\n",
    "max_depth = 20\n",
    "alpha = 11\n",
    "learning_rate = 0.3\n",
    "\n",
    "# XGBooost Random Forest Regresor Model \n",
    "xgb_reg_model = xgb.XGBRFRegressor(objective='reg:squarederror', colsample_bytree=1, \n",
    "                                    min_child_weight=2, max_depth=max_depth, \n",
    "                                    learning_rate=learning_rate, tree_method='hist', \n",
    "                                    n_estimators=n_est, alpha=alpha)\n",
    "\n",
    "print(\"Beginning to Train XGBoost Model\")\n",
    "\n",
    "# My own personal curiosity into comparing how long my models take to train \n",
    "start = time.time()\n",
    "xgb_reg_model.fit(x_train_pp, y_train)\n",
    "end = time.time()\n",
    "print('\\n')\n",
    "if (end-start)>=60:\n",
    "    print(\"Training took approx. \" + str((end-start)/60) + \" minutes\")\n",
    "else:\n",
    "    print(\"Training took approx. \" + str(end-start) + \" seconds\")\n",
    "\n",
    "# Make predictions on the testing set \n",
    "preds = xgb_reg_model.predict(x_test_pp)\n",
    "preds = np.absolute(preds)\n",
    "\n",
    "# Compute the target metric, in this case RMSLE \n",
    "print('RMSLE: ', np.sqrt(mean_squared_log_error(y_test, preds)))\n",
    "print(\"N_estimators :\", n_est)\n",
    "print(\"Max_Depth: \", max_depth)\n",
    "print(\"Alpha Val: \", alpha)\n",
    "print(\"Learning Rate: \", learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to Train LightGBM Model\n",
      "\n",
      "\n",
      "Training took approx. 5.595957279205322 seconds\n",
      "RMSLE:  3.0893955867738305\n",
      "N_estimators : 20\n",
      "Max_Depth:  20\n",
      "Alpha Val:  11\n",
      "Learning Rate:  0.4\n"
     ]
    }
   ],
   "source": [
    "lgb_n_est = 20\n",
    "num_leaves = 100\n",
    "lgb_depth = 10\n",
    "min_data_in_leaf = 500\n",
    "\n",
    "# LightGBM Regression Model\n",
    "LightGBM = lgb.sklearn.LGBMRegressor(boosting_type='gbdt', n_estimators=lgb_n_est, \n",
    "                                    num_leaves=num_leaves, max_depth=lgb_depth, \n",
    "                                     min_data_in_leaf = min_data_in_leaf)\n",
    "\n",
    "print('Beginning to Train LightGBM Model')\n",
    "\n",
    "start = time.time()\n",
    "LightGBM.fit(x_train_pp, y_train)\n",
    "end = time.time()\n",
    "\n",
    "print('\\n')\n",
    "if (end-start)>=60:\n",
    "    print(\"Training took approx. \" + str((end-start)/60) + \" minutes\")\n",
    "else:\n",
    "    print(\"Training took approx. \" + str(end-start) + \" seconds\")\n",
    "    \n",
    "# Make Predictions on the testing set with LightGBM model \n",
    "Light_Preds = LightGBM.predict(x_test_pp)\n",
    "Light_Preds = np.absolute(Light_Preds)\n",
    "\n",
    "# Compute the target metric for the LightGBM Predictions \n",
    "print('RMSLE: ', np.sqrt(mean_squared_log_error(y_test, Light_Preds)))\n",
    "print(\"N_estimators :\", lgb_n_est)\n",
    "print(\"Max_Depth: \", lgb_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatB_Reg = CatBoostRegressor(iterations=10, learning_rate=0.4, \n",
    "                            depth=10, loss_function='RMSE', l2_leaf_reg=11)\n",
    "\n",
    "print(\"Beginning to Train CatBoost Model\")\n",
    "\n",
    "start = time.time()\n",
    "CatB_Reg.fit(x_train_pp, y_train)\n",
    "end = time.time()\n",
    "\n",
    "print('\\n')\n",
    "if (end-start)>=60:\n",
    "    print(\"Training took approx. \" + str((end-start)/60) + \" minutes\")\n",
    "else:\n",
    "    print(\"Training took approx. \" + str(end-start) + \" seconds\")\n",
    "\n",
    "# Make Predictions on the testing set with CatBoost model \n",
    "Cat_Preds = CatB_Reg.predict(x_test_pp)\n",
    "# Cat_Preds = np.absolute(Light_Preds)\n",
    "\n",
    "# Compute the target metric for the LightGBM Predictions \n",
    "print('RMSLE: ', np.sqrt(mean_squared_log_error(y_test, Cat_Preds)))\n",
    "print(\"N_estimators :\", n_est)\n",
    "print(\"Max_Depth: \", max_depth)\n",
    "print(\"Alpha Val: \", alpha)\n",
    "print(\"Learning Rate: \", learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
